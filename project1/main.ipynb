{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibilityof the statistical analisis of the mortaliy rate after hurricane maria\n",
    "## Team:\n",
    "__Angel A. Sanquiche Sanchez 801-13-7075__\n",
    "\n",
    "__Jonathan J. Lozada Pérez 401-14-3675__\n",
    "\n",
    "__Gabriela Lozano 801-17-2927__\n",
    "## Requirements:\n",
    "\n",
    "__Tools:__ Analysis will be done in python, using a number of useful open source python packages:\n",
    "- __Requests__ Requests is the only Non-GMO HTTP library for Python, safe for human consumption.\n",
    "- __PyPDF2__ A Pure-Python library built as a PDF toolkit.\n",
    "\n",
    "__Anaconda__ https://www.anaconda.com/download/\n",
    "__Jupyter__ https://jupyter.org/\n",
    "\n",
    "Most of these packages come bundled as part of the Anaconda python distribution. Install Anaconda for the simplest way to get all dependancies for the tutorial: __Anaconda__ https://www.anaconda.com/download/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Wrangling\n",
    "## Necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the necesary libraries to work with the PDF\n",
    "#!pip install requests #this might come with a standard anaconda install\n",
    "#!pip install PyPDF2 #we will use this to scrape text out of the pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aquire the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "pdf = r\"https://github.com/c2-d2/pr_mort_official/raw/master/data/RD-Mortality-Report_2015-18-180531.pdf\"\n",
    "\n",
    "open(\"Mortality.pdf\" , 'wb').write(requests.get(pdf).content)\n",
    "\n",
    "from IPython.display import IFrame#insert the pdf into the notebook\n",
    "IFrame(src=\"Mortality.pdf\" ,width=700 ,height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "pdfFileObj = open('Mortality.pdf', 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "pageObj = pdfReader.getPage(0)\n",
    "#pageObj.extractText() #uncomment to see alphabet soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is to adjust the months to have an order to them\n",
    "MonthToNumber = {\n",
    "    \"SEP\":\"09\",\n",
    "    \"OCT\":\"10\",\n",
    "    \"NOV\":\"11\",\n",
    "    \"DEC\":\"12\",\n",
    "    \"JAN\":\"01\",\n",
    "    \"FEB\":\"02\",\n",
    "    \"MAR\":\"03\",\n",
    "    \"APR\":\"04\",\n",
    "    \"MAY\":\"05\",\n",
    "    \"JUN\":\"06\",\n",
    "    \"JUL\":\"07\",\n",
    "    \"AGO\":\"08\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(data , month):\n",
    "    data = data[1].split()\n",
    "    labels = data[0:4]\n",
    "    indexes = []\n",
    "    stop = data.index('Total')\n",
    "    out = []\n",
    "    for i in range(4 , stop , 5):\n",
    "        indexes.append(data[i])\n",
    "        day = data[i]\n",
    "        if not len(day)-1:\n",
    "            day = \"0\"+day\n",
    "        if (i != 144 or month!=\"FEB\"):#Nuts to whoever decided that the months arent all the same\n",
    "            \n",
    "            out.append([F\"{labels[0]}-{MonthToNumber[month]}-{day}\",int(data[i+1])])\n",
    "            out.append([F\"{labels[1]}-{MonthToNumber[month]}-{day}\",int(data[i+2])])\n",
    "            out.append([F\"{labels[2]}-{MonthToNumber[month]}-{day}\",int(data[i+3])])            \n",
    "            out.append([F\"{labels[3]}-{MonthToNumber[month]}-{day}\",int(data[i+4])])\n",
    "        else:\n",
    "            #print(\"Breaking\")\n",
    "            out.append([F\"{labels[1]}-{MonthToNumber[month]}-{day}\",int(data[i+1])])\n",
    "            break\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(txt):\n",
    "    data = []\n",
    "    months = [\"SEP\", \"OCT\", \"NOV\", \"DEC\" , \"JAN\", \"FEB\", \"MAR\", \"APR\", \"MAY\" , \"JUN\" , \"JUL\", \"AGO\"]\n",
    "    for month in months:\n",
    "        if month in txt:\n",
    "            \"\"\"if \"FEB\"==month:\n",
    "                return feb_scrape(txt.split(month))\n",
    "            else:\n",
    "                return general_scrape(txt.split(month))\"\"\"\n",
    "            return scrape_data(txt.split(month) , month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for pages in range(pdfReader.getNumPages()):\n",
    "    data += scrape(pdfReader.getPage(pages).extractText())\n",
    "data.sort()\n",
    "for frame in data[:10]:\n",
    "    print(frame)\n",
    "for frame in data[-10:]:\n",
    "    print(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(data, open( \"df.pkl\", \"wb\" ) )\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Raw Data Visualization\n",
    "## Requirements to import raw data\n",
    "__Tools:__ Analysis will be done in python, using a number of useful open source python packages:\n",
    "- __Pandas__ A very useful data analysis and manipulation package http://pandas.pydata.org/\n",
    "- __Numpy__ Python's scientific computing package. http://www.numpy.org/\n",
    "- __ggplot__ Python implementation of the grammar of graphics created for R https://pypi.org/project/ggplot/\n",
    "- __Matplotlib__ Package for creating charts and other visualizations https://matplotlib.org/\n",
    "- __datetime__ Module that supplies classes for manipulating dates and times https://docs.python.org/3/library/datetime.html\n",
    "\n",
    "All these packages come bundled as part of the Anaconda python distribution. Install Anaconda for the simplest way to get all dependancies for the tutorial:\n",
    "\n",
    "\n",
    "Let's get started by importing the raw data and libraries we will use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import pickle library to import pickled file\n",
    "import pickle\n",
    "data = pickle.load( open( \"df.pkl\", \"rb\" ) )\n",
    "\n",
    "###################IMPORTANT####################\n",
    "#site-packages/pandas/lib.py needs to be edited#\n",
    "#change:from pandas_lib._lib import *          #\n",
    "#    to:from pandas import *                   #\n",
    "################################################\n",
    "\n",
    "#Import libraries to handle data and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ggplot import *\n",
    "import matplotlib as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data to dataframe\n",
    "Now, we convert raw data file into a manageable pandas dataframe and filter the data we will actually plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create arrays to allocate dates and deaths from raw data file\n",
    "dates, deaths = [], []\n",
    "\n",
    "#Append dates and deaths to corresponding array\n",
    "for a, b in data:\n",
    "    dates.append(a)\n",
    "    deaths.append(b)\n",
    "\n",
    "#Convert dates to datetime object for easy handling\n",
    "dates = [pd.to_datetime(d) for d in dates]\n",
    "\n",
    "#Merge both arrays into dataframe\n",
    "df = pd.DataFrame(list(zip(dates, deaths)), index = dates, columns = ('Dates', 'Deaths'))\n",
    "\n",
    "#Filter dates and deaths to plot\n",
    "dates = df.loc['2015-01-01':'2018-05-30']\n",
    "deaths = df['Deaths'] > 0\n",
    "\n",
    "#Create new dataframe from filters\n",
    "df_2 = df[deaths]\n",
    "df_2 = df.loc['2017-01-01':'2018-05-30']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter plot of raw data for 2017-2018 deaths in Puerto Rico\n",
    "Lastly, we use ggplot to create a scatter plot with the data of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create scatter plot of Dates vs Deaths from January, 1, 2017-May, 30, 2018 \n",
    "#data for observations with more than zero(0) deaths\n",
    "g = (ggplot(aes(x = 'Dates', y = 'Deaths'), data = df_2) +\\\n",
    "geom_point(alpha = 0.5, color = \"black\", size = 75) +\\\n",
    "ggtitle(\"Raw Data Dates vs Deaths 2017-2018 (Python)\") +\\\n",
    "scale_x_date(labels = \"%b\", date_breaks = \"1 months\", date_minor_breaks = \"1 months\") +\\\n",
    "scale_y_continuous(limits = (0, 150),\n",
    "        labels = labs(50, 150, 50),\n",
    "        breaks = range(0, 150, 50)) +\\\n",
    "#Intercept line at date of María Hurricane and mean deaths per day, respectively\n",
    "geom_vline(x = [pd.to_datetime(datetime.date(2017,9,20))], linetype = \"dashed\", color = \"tomato\", size = 3) +\\\n",
    "geom_hline(y = df_2['Deaths'].mean(), linetype = \"dashed\", color = \"cornflowerblue\", size = 3))\n",
    "\n",
    "print(\"Mean death count per day: \", df_2['Deaths'].mean())\n",
    "print(\"Hurricane María Date: \", df_2.loc['2017-09-20', 'Dates'])\n",
    "print(\"Hurricane María Deaths:\", df_2.loc['2017-09-20', 'Deaths'])\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Data Analysis\n",
    "\n",
    "__Tools__: Analysis will be done in python, using a number of useful open source python packages:\n",
    "\n",
    "- __Pandas__ A very useful data analysis and manipulation package http://pandas.pydata.org/\n",
    "- __Numpy__ Python's scientific computing package. http://www.numpy.org/\n",
    "- __Statsmodels__ is a module that provides classes and functions for the estimation of many different statistical models. https://www.statsmodels.org/\n",
    "\n",
    "\n",
    "\n",
    "Let's get started by importing the raw data and libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataframe and add the date columns\n",
    "Now, the data is more manageable and we can work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame is imported as a csv from the wrangling and made into a dataframe with the date as index\n",
    "df = pd.read_csv('mortality.csv', parse_dates = ['date'])\n",
    "df = df.set_index(df.date)\n",
    "\n",
    "# Adding year, month, day as columns in the dataframe\n",
    "df['year'] = pd.DatetimeIndex(df['date']).year\n",
    "df['month'] = pd.DatetimeIndex(df['date']).month\n",
    "df['day'] = pd.DatetimeIndex(df['date']).day\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making dictionaries from estimates and adding it to our data set\n",
    "The website https://simplystatistics.org/2018/06/08/a-first-look-at-recently-released-official-puerto-rico-death-count-data/ includes estimates from Teralytics and estimates of population by year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# population from yearly data and data sampled from Teralytics estimates by author \n",
    "# this is made into a dictionary \n",
    "samples = {\n",
    "    '2010-07-02': 3721525,\n",
    "    '2011-07-02': 3678732,\n",
    "    '2012-07-02': 3634488,\n",
    "    '2013-07-02': 3593077,\n",
    "    '2014-07-02': 3534874,\n",
    "    '2015-07-02': 3473177,\n",
    "    '2016-07-02': 3406520,\n",
    "    '2017-07-02': 3337177,\n",
    "    \n",
    "    '2017-09-19': 3337000,\n",
    "    '2017-10-15': 3237000,\n",
    "    '2017-11-15': 3202000,\n",
    "    '2017-12-15': 3200000,\n",
    "    '2018-01-15': 3223000,\n",
    "    '2018-02-15': 3278000\n",
    "}\n",
    "\n",
    "# add the above data points to our data set\n",
    "# turn the dictionary into a datafrane and add it into the original data frame\n",
    "# index is date and the column is est_pop\n",
    "pdf = pd.DataFrame.from_dict(samples, orient='index', columns=['est_pop'])\n",
    "pdf = pdf.set_index(pd.to_datetime(pdf.index))\n",
    "pdf.est_pop = pdf.est_pop.astype(int)\n",
    "\n",
    "df = df.join(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear interpolation of estimated population\n",
    "Linear interpolation is a method of curve fitting using linear polynomials to construct new data points within the range of a discrete set of known data points. This estimate is used to graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear interpolation is method of curve fitting using linear polynomials to construct \n",
    "# new data points within the range of a discrete set of known data points \n",
    "df['est_pop_lerped'] = df.est_pop.interpolate(limit_direction='both')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Rate\n",
    "Using days prior to sept 20th to calculate yearly median death rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# made a copy of the dataframe\n",
    "rdf = df.copy()\n",
    "\n",
    "# the article says only to use days prior to sept 20th in calcualting yearly median death rate\n",
    "rdf = rdf[rdf['date'].dt.dayofyear < 263] # sept 20th is 263rd day of year \n",
    "\n",
    "# calculating calc_rate\n",
    "rdf['calc_rate'] = rdf.deaths / rdf.est_pop_lerped * 365 * 1000\n",
    "rdf.groupby('year').median() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating yearly rates\n",
    "Now, we use the data given to us by the page previously mentioned and add it into our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary given by the original website\n",
    "yearly_rates = {\n",
    "    2015: 8.197106,\n",
    "    2016: 8.387468,\n",
    "    2017: 8.750266,\n",
    "    2018: 8.750266 # this is assumed same as 2017 from above result\n",
    "}\n",
    "\n",
    "# yrdf= yearly rates data frame\n",
    "# turn the dictionary into a data frame and add to dataframe\n",
    "yrdf = pd.DataFrame.from_dict(yearly_rates, orient='index', columns=['year_rate'])\n",
    "df = df.join(yrdf, on='year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate day rate\n",
    "Adding a daily rate column calculating the death rate across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculate death rate across entire dataset\n",
    "df['day_rate'] = df.deaths / df.est_pop_lerped * 365 * 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up population columns\n",
    "Cleaning up the columns to make it more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up population columns\n",
    "df = df.drop('est_pop', axis=1)\n",
    "df = df.rename({\"est_pop_lerped\": \"pop\"}, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate day rate and average rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = df.query('year < 2017 and not (month == 2 and day == 29)')\n",
    "ardf = adf.groupby(['month', 'day'], as_index=False).apply(lambda x: (x.day_rate - x.year_rate).mean())\n",
    "ardf = ardf.to_frame(name='avg_rate')\n",
    "ajdf = adf.merge(ardf, on=['month', 'day'])\n",
    "\n",
    "adf = ajdf \n",
    "adf.plot(x='date', y='avg_rate', style='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average rate smoothed\n",
    "Using lowess to calculate the average rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to run and use lowess to smoothen out graph\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import statsmodels.api as sm\n",
    "lowess = sm.nonparametric.lowess\n",
    "\n",
    "smdf = adf\n",
    "smdf['avg_rate_smoothed'] = lowess(adf.avg_rate, adf.date, frac=0.15, it=3, return_sorted=False)\n",
    "smdf.plot(x='date', y=['avg_rate', 'avg_rate_smoothed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate trend\n",
    "Merge some columns and add it to a new dataframe that includes all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging and renaming\n",
    "full = pd.merge(df, smdf[['date', 'avg_rate', 'avg_rate_smoothed']], on='date', how='left')\n",
    "full = full.set_index(pd.to_datetime(full.date))\n",
    "\n",
    "full = full.rename({\"avg_rate_smoothed\": \"trend\", \"day_rate\": \"rate\"}, axis=1)\n",
    "\n",
    "# TODO: remove this\n",
    "full.plot(x='date', y=['year_rate', 'rate', 'trend'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annual trend\n",
    "Calculate annual trend using the data from 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy annual trend to 2017-18\n",
    "ann_trend = full.loc[full[\"date\"].dt.year == 2015, \"trend\"]\n",
    "ann_trend # how to copy to 2017-18??\n",
    "ann_trend = pd.DataFrame(ann_trend)\n",
    "ann_trend.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
